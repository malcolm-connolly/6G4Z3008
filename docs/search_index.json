[["index.html", "Probability Theory and Statistics Chapter 1 Introduction to Probability 1.1 Frequentist perspective 1.2 Naive probability 1.3 Complements and mutual exclusivity 1.4 Outcomes and counting 1.5 Exercises (Week 1)", " Probability Theory and Statistics Malcolm Connolly Semester 2, 2023 Chapter 1 Introduction to Probability Some things that happen are entirely predictable. For example, if one drops a ball from a height, we know it will hit the ground. Things that happen like this can be decribed as deterministic. You may have heard people talk about things being written in the stars, or their fate, or destiny. The opinion that all things are pre-determined is called determinism. However, even if are a determinist, you will have to live with uncertainty. In our everyday lives we can think of examples where things happen that we cannot predict; a bus may be late, it may rain, or one might win the lottery. To one living with uncertainty, it is reasonable to quantify this uncertainty and act assuming outcomes are not pre-determined. If the outcome is not pre-determined then it is called random. The Mathematics of random phenomena is called Probability Theory. Most people have an intuitive idea of what is meant by probability or chance. Unfortunately Probability Theory is a subject in which there are endless examples of seemingly simple questions that turn out to be very complicated or have severely counter-intuitive answers. 1.1 Frequentist perspective We need to start with some terminology. Definition 1.1 An experiment is any procedure which happens at random with at least two different outcomes. For example rolling a die and observing the score is a statistical experiment. If the experiment is repeatable then each repetition is called a run. By calculating the number of times an event occurs divided by the number of runs one can estimate the theoretical probability. The idea is that the relative cumulative frequency of outcomes will tend to the actual probability in the long run. This is perspective of probability is called Frequentist, and is incredibly useful in practice. Figure 1.1: The result of simulating rolling a die 6000 times, and counting how many times 6 occures. The cumulative relative frequency tends to the theoretical 1/6 (in red). We will recreate a plot like this in labs. Example 1.1 Suppose we toss a \\(10\\) coins \\(10\\) times and the results are recorded in the table below, draw the graph of relative frequency. Run 1 2 3 4 5 6 7 8 9 10 Outcome 6H 3H 3H 1H 6H 3H 6H 5H 5H 7H The cumulative relative frequencies are calculated as the cumulative number of flips divided by the cumulative number of heads: Cumulative flips \\(n\\) 10 20 30 40 50 60 70 80 90 100 Cumulative heads \\(a_n\\) 6 9 12 13 19 22 28 33 38 45 Relative Frequency 0.6 0.45 0.4 0.325 0.38 0.367 0.4 0.413 0.422 0.45 In this course we will learn some R programming. R is a free open-source software language suitable for doing many probability and statistical calculations. The following R code will make a list of two outcomes Heads or Tails and create a sample of \\(10\\) random outcomes. outcomes &lt;- c(&quot;Heads&quot;,&quot;Tails&quot;) sample(outcomes, 10, replace=TRUE) ## [1] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; ## [10] &quot;Tails&quot; Definition 1.2 If a statistical experiment has \\(n\\) runs, and the outcome \\(A\\) happens a cumulative number of times depending on \\(n\\) which we can call \\(a_n\\), then the frequentist probability of the outcome \\(A\\), written \\(P(A)\\), is the limit: \\[P(A) = \\lim_{n\\to \\infty} \\frac{a_n}{n}\\] So if it is possible to repeatedly run an experiment, frequentist methods are very useful for finding an approximation of the true theoretical probability. Not all is so simple, consider the following questions. What is the probability that there is life on other planets? What is the probability that the Conservatives win the next general election? These events are not like flipping a coin, and so it is not possible to find a frequentist interpretation for their probability. 1.2 Naive probability We may not have the time or resources to do many thousands of runs. Therefore we also need to be able to evaluate the theoretical probability directly and exactly. Definition 1.3 The sample space is a set whose elements are outcomes of an experiment. The sample space is denoted by the greek letter \\(\\Omega\\). Example 1.2 If we pick a person at random on the street and ask them the month of their birthday, we can let \\[\\Omega = \\{\\text{Jan}, \\ \\text{Feb}, \\ \\text{Mar}, \\ \\text{Apr}, \\ \\text{May}, \\ \\text{Jun}, \\ \\text{Jul}, \\ \\text{Aug}, \\ \\text{Sep}, \\ \\text{Oct}, \\ \\text{Nov}, \\ \\text{Dec} \\}.\\] Definition 1.4 An event is a subset of the sample space \\(\\Omega\\). Example 1.3 As in example 1.2, let \\(\\text{L}\\) be the event that the month is a long month (i.e. has 31 days). Then \\[\\text{L} = \\{\\text{Jan}, \\ \\text{Mar}, \\ \\text{May}, \\ \\text{Jul}, \\ \\text{Aug}, \\ \\text{Oct}, \\ \\text{Dec} \\}.\\] Let \\(R\\) be the event that there is a letter r in the name of the month when written fully. Here, \\[\\text{R} = \\{\\text{Jan}, \\ \\text{Feb}, \\ \\text{Mar}, \\ \\text{Apr}, \\ \\text{Sep}, \\ \\text{Oct}, \\ \\text{Nov}, \\ \\text{Dec} \\}\\] Definition 1.5 Naively the the probability of an event \\(A\\) should be the number of elements of the set \\(A\\) divided by the size of the sample space \\(\\Omega\\).That is, \\(\\text{P} (A) = \\frac{|A|}{|\\Omega|}\\). In our example 1.3 above: \\[\\text{P}(R) = \\frac{|R|}{|\\Omega|} = \\frac{8}{12} = \\frac{2}{3},\\] and, \\[\\text{P}(L) = \\frac{|L|}{|\\Omega|} =\\frac{7}{12}.\\] Example 1.4 (Coin Tossing) Toss a fair coin twice and record the possible outcomes. Let \\[A = \\{\\text{exactly one coin is Heads}\\}\\] and \\[B = \\{\\text{neither coin is Heads}\\}\\] The sample space here is \\(\\Omega = \\{HH, HT, TH, HH\\}\\). Events \\(A\\) and \\(B\\) correspond to: \\[A = \\{HT, TH\\}\\] and \\[B = \\{ TT \\}\\] Hence \\(\\text{P}(A) = \\frac{2}{4} = \\frac{1}{2}\\), and \\(\\text{P}(B)=\\frac{1}{4}\\). Example 1.5 (Two dice) Two dice are thrown, what is the probability that the total number of dots is: equal to \\(7\\) equal to \\(3\\) greater than \\(5\\) an even number solution The sample space here is \\(\\Omega = \\{ (n_1,n_2) : n_1 , n_2 \\in \\{1,2,3,4,5,6 \\} \\}\\). However, not all sums are equally likely, which is best seen in a table. 1 2 3 4 5 6 1 2 3 4 5 6 7 2 3 4 5 6 7 8 3 4 5 6 7 8 9 4 5 6 7 8 9 10 5 6 7 8 9 10 11 6 7 8 9 10 11 12 \\(\\frac{6}{36}\\) \\(\\frac{2}{36}\\) \\(\\frac{26}{36}\\) \\(\\frac{18}{36}\\) For infinite sets there is a problem with the naive definition 1.5. Consider the following: Example 1.6 Suppose a random unit vector is rotated about the origin anticlockwise, making an angle \\(\\theta\\) with the positive \\(x\\)-axis. What is the probability that this angle is acute? There are a continuum of infinitely many such angles. The naive definition says \\(\\frac{\\infty}{\\infty}\\), which is absurd. Intuitively, the answer should be \\(\\frac{1}{4}\\). 1.3 Complements and mutual exclusivity In any case, as events are subsets of the sample space \\(\\Omega\\) and follow the rules of set theory, and so it is important to know some set notation, definitions and results. Below is a recap of the important definitions. Definition 1.6 The union of \\(A\\) and \\(B\\) is written: \\[A\\cup B = \\{ x \\in \\Omega : x \\in A \\ \\text{or} \\ x\\in B \\}.\\] In Mathematics or is inclusive, which means we do not need to say ``or both as this is included in the union. Definition 1.7 The intersection of \\(A\\) and \\(B\\) is written: \\[A\\cap B = \\{ x \\in \\Omega: x \\in A \\ \\text{and} \\ x\\in B \\}.\\] Definition 1.8 The empty set \\(\\varnothing\\) is the set of no elements. As sets \\(A\\) and \\(B\\) are called disjoint if they have no elements in common, that is, \\(A \\cap B = \\varnothing.\\) In Probability Theory disjoint events are called mutually exclusive. Definition 1.9 The complement of an event \\(A\\) is the event \\(A^{c} = \\{x \\in \\Omega : x\\notin A\\}.\\) Note $A A^{c} = . In words this means: any event is mutually exclusive with its complement. Example 1.7 Suppose the event is throwing a die. The event is that one throws an even number. The complement is that one throws an odd number. Example 1.8 Suppose the event is that a random student has no siblings. The complement is not that they have one sibling. The complement is that they have at least one sibling. A theorem which we will not prove is De Morgans laws Theorem 1.1 (DE MORGAN'S LAWS) The complement of a union is the intersection of the complements: \\[(A \\cup B)^{c} = A^{c} \\cap B^{c}\\] The complement of an intersection is the union of the complements: \\[(A \\cap B)^{c} = A^{c} \\cup B^{c}\\] In this way \\(P\\) is a `measure function which maps the subsets of the sample space to the interval \\(\\left[0,1\\right]\\). Definition 1.10 Probability is a function whose input is a subset of the sample space \\(A \\subseteq \\Omega\\) and whose range is the interval \\(\\left[0,1\\right]\\), such that the following two axioms hold: The probability of the whole set of possible events is unity. In the notation: \\(\\text{P}(\\Omega ) =1\\). (additivity) For any collection of disjoint events \\(A_1 , A_2, A_3, \\dots\\) the probability of the union is the sum of the probabilities. In the notation this can be written as \\[\\text{P}(A_1 \\cup A_2 \\cup \\dots ) = \\text{P}(A_1) + \\text{P}(A_2)+\\dots .\\] The above definition 1.10 is due to the Russian Mathematician Kolmogorov. These axioms help make sense of the infinite case. Using this definition we can prove the following important results. Proposition 1.1 (THE PROBABILITY OF A COMPLEMENT) For any event \\(A\\) we have: \\[\\text{P}(A^{c}) = 1 - \\text{P}(A).\\] Proof. Write \\(\\Omega = A \\cup A^{c}\\), which is a disjoint union. Then by additivity, \\[\\text{P}(\\Omega) = \\text{P}(A) + \\text{P}(A^{c}) \\] Now by axiom (i) the LHS is \\(1\\). Theorem 1.2 (THE PROBABILITY OF A UNION) Given any two events \\(A\\) and \\(B\\) we have: \\[\\text{P}(A\\cup B) = \\text{P}(A) + \\text{P}(B) - \\text{P}(A \\cap B)\\] Proof. The idea is to write \\(A\\) as a disjoint union of the part that has intersection with \\(B\\), and that which does not: \\(A=(A\\cap B)\\cup(A\\cap B^{c})\\). Hence, \\[\\text{P}(A) = \\text{P}(A\\cap B) + \\text{P}(A\\cap B^{c})\\] If we split \\(A\\cup B\\) in the same way, we obtain \\((A\\cup B)\\cap B\\) and \\((A\\cup B)\\cap B^{c}\\). The former is simply \\(B\\), and the latter is \\(A \\cap B^{c}\\). Again by additivity, \\[\\text{P}(A \\cup B) = P(B) + P(A\\cap B^{c}).\\] Eliminating \\(P(A\\cap B^{c})\\) from the two equations above proves the rule. We will not be proving all Theorems in this course, neither will I ask you to recount a proof in an exam. You will however have to know how to use these results in applied problems. Example 1.9 (Multiple Choice) Suppose a multiple choice test consists of three questions each of which has two options, the correct answer (C) or the wrong answer (W). What is the probability that a student who always randomly guesses the answers gets at least one correct? \\[\\begin{align} \\text{P(at least one correct)} &amp;= 1 - \\text{P(all wrong)} \\\\ &amp;= 1- \\frac{1}{8} \\\\ &amp;=\\frac{7}{8} \\end{align}\\] Example 1.10 (Mode of travel) The table shows the type of journey undertaken by a sample of commuters classified by where they live. Town Rural Car 40 30 70 Bus 25 5 30 65 35 100 If an individual is selected at random from this group, find the probability that, they travel by car or live in the town solution \\(\\text{P}(\\text{Car}\\cup \\text{Town}) = \\frac{25+40+30}{100}=0.95\\) \\(\\text{P}(\\text{Car})+ \\text{P}(\\text{Town})-\\text{P}(\\text{Car}\\cap \\text{Town})= \\frac{65}{100}+\\frac{70}{100}-\\frac{40}{100} =0.95\\) Example 1.11 In a particular city \\(60\\%\\) of people watch the news in the morning, \\(50\\%\\) of people watch the news in the evening and \\(30\\%\\) watch both. What is the probability that an individual selected at random watches either the morning news or the evening news. solution \\(\\text{P}(M\\cup E) = 0.6 + 0.5 - 0.3 = 0.8\\) 1.4 Outcomes and counting One might imagine that the finite situation is then very simple, and even then we have seen this is not the full picture. One simply counts how many ways an event can happen out of the total number of configurations. This can actually be quite complicated. We will learn some formulae to enable us to count them. 1.4.1 Factorials Example 1.12 (Three people in a line) In how many ways can three people \\(A\\), \\(B\\) and \\(C\\) stand in a line? solution \\(ABC, ACB, BAC, BCA, CAB,CBA\\) there are \\(6\\). Definition 1.11 For any non-negative integer, \\(n\\) say, we define the factorial of \\(n\\), written \\(n!\\) to be equal to the product of \\(n\\) and all the numbers less than \\(n\\) down to \\(1\\). That is, \\[n! = n \\times (n-1) \\times (n-2) \\times \\dots 3 \\times 2 \\times 1\\] Definition 1.12 (Multiplication Rule) If there are \\(n\\) ways for some operation to happen, and \\(m\\) ways for something else to happen, then the total number of ways for the sequence to occur is \\(n \\times m\\). Example 1.13 (Rule 1) The number of ways of arranging \\(n\\) distinct objects in a line is \\(n!\\). This is because there are \\(n\\) choices for the first number in line, then one fewer choice \\((n-1)\\) for the second, and so on, until the last one in the line there is only one choice remaining. Example 1.14 MMU assigns each student an \\(8\\) digit ID number. How many possible ID numbers are there? solution The first digit is not zero, there are \\(9\\) digits from which to choose. All the other digits have \\(10\\) choices \\(0,1,2,3,4,5,6,7,8,9\\). Total = \\(9 \\times 10^7\\). Definition 1.13 (rule of sum) Given two disjoint events \\(A\\) and \\(B\\), then the size of the union is the sum of the sizes of \\(A\\) and \\(B\\). That is, \\[|A\\cup B|=|A|+|B|\\] Example 1.15 How many possible MMU IDs start with a \\(1\\) or a \\(3\\)? solution The IDs are all of the form 1******* or 3*******. There is only 1 choice for the first digit and \\(10^7\\) choices for the next digits in either case. The total number starting with a \\(1\\times 10^7 + 1\\times 10^7 = 2\\times 10^7.\\) ::: {.definition name=Rule 2} 1.4.2 Permutations Example 1.16 Suppose I want to hack someones \\(4\\) digit PIN, and they tell me the digits are all different. I need to choose \\(4\\) numbers from \\(10\\), but for each \\(4\\) that I choose they could happen in any order. For example \\(1,2,3,4\\) could also occur as The number of ways of choosing \\(k\\) distinct items from \\(n\\) when the order is relevant is \\[^n\\text{P}_k = \\frac{n!}{(n-k)!}\\] ### Combinations 1.5 Exercises (Week 1) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
